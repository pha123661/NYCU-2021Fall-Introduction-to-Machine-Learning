{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5 - Artificial Neural Networks\n",
    "---\n",
    "## Name: 李勝維\n",
    "## Student ID: 0711239\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import seed_everything\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "seed_everything(0)\n",
    "plt.rcParams['figure.dpi'] = 200 # 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input & Data Visualization\n",
    "\n",
    "Use package 'json' to read and 'pandas' to save data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "# Data input\n",
    "with open('2021-nycu-ml-hw5\\\\train.json', 'r') as file:\n",
    "    raw_data = json.load(file)\n",
    "raw_data = [(x['id'], x['cuisine'], x['ingredients']) for x in raw_data]\n",
    "ids, cuisines, ingredientss = zip(*raw_data)\n",
    "df = pd.DataFrame({\n",
    "    'id':ids,\n",
    "    'cuisine':cuisines,\n",
    "    'ingredients':ingredientss,\n",
    "})\n",
    "\n",
    "# Data Visualization\n",
    "# label\n",
    "count = df['cuisine'].value_counts()\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.title(\"Value count of labels\")\n",
    "plt.bar(count.index, height=count.values)\n",
    "plt.show()\n",
    "\n",
    "# features\n",
    "count = defaultdict(lambda : 0)\n",
    "for x in df['ingredients']:\n",
    "    for n in x:\n",
    "        count[n] += 1\n",
    "# import pprint;pprint.pprint(count)\n",
    "# get Top 30 features\n",
    "count = {k: v for idx, (k, v) in enumerate(sorted(count.items(), key=lambda item: item[1], reverse=True)) if idx < 30}\n",
    "count = pd.Series(count)\n",
    "plt.figure(figsize=(40,15))\n",
    "plt.title(\"Value count of labels\")\n",
    "plt.bar(count.index, height=count.values)\n",
    "plt.show()\n",
    "\n",
    "# number of ingredients\n",
    "count = defaultdict(lambda : 0)\n",
    "for x in df['ingredients']:\n",
    "    count[len(x)] += 1\n",
    "count = {k: v for (k, v) in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "count = pd.Series(count)\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.title(\"Value count of number of features\")\n",
    "plt.bar(count.index, height=count.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "資料預處理步驟：\n",
    "1. Shuffle  \n",
    "2. 去除 'stop words' (EX: the, any, then ...) 沒意義的單字\n",
    "3. 轉成小寫 (不區分大小寫)\n",
    "4. 使用 Porter Stemmer 演算法去除單字變化 (EX: 單複數變化)\n",
    "5. 最後計算 TF-IDF 矩陣來取代 one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def Porter_Stemmer(s):\n",
    "    PS = PorterStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    str = ' '.join([PS.stem(w.lower()) for w in s if w not in stop_words])\n",
    "    return str\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "\n",
    "# Porter Stemmer\n",
    "df['concat'] = [Porter_Stemmer(s) for s in df['ingredients']]\n",
    "\n",
    "# Transform into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['concat']).toarray().astype(np.float32)\n",
    "# Encode Y\n",
    "y_encoder = LabelEncoder()\n",
    "df['cuisine'] = y_encoder.fit_transform(df['cuisine'])\n",
    "y = df['cuisine'].values.astype(np.longlong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "## Reasoning of model choice:\n",
    "選擇MLP來做只是因為它實作最快，效果也有保證，多次實驗不同的架構/有無batch norm/有無dropout後，選擇只有兩層的架構，因為訓練資料不夠多，參數量少一點比較好\n",
    "\n",
    "## Data augmentation\n",
    "無\n",
    "\n",
    "## Training process\n",
    "MLP的輸出層有20個node，和label種類相同，直接使用cross entropy作為loss function進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self.clf = nn.Sequential(\n",
    "        #     nn.Linear(2809, 4096),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.BatchNorm1d(4096),\n",
    "        #     nn.Dropout(),\n",
    "\n",
    "        #     nn.Linear(4096, 2048),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.BatchNorm1d(2048),\n",
    "        #     nn.Dropout(),\n",
    "\n",
    "        #     nn.Linear(2048, 1024),\n",
    "        #     nn.LeakyReLU(0.2),\n",
    "        #     nn.BatchNorm1d(1024),\n",
    "        #     nn.Dropout(),\n",
    "\n",
    "        #     nn.Linear(1024, 20),\n",
    "        # )\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(2809, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(512, 20),\n",
    "        )\n",
    "        self.criterion = nn.functional.cross_entropy\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x).to(next(iter(self.parameters())).device)\n",
    "        rst = self.clf(x)\n",
    "        return torch.argmax(rst, 1).cpu().numpy()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self.clf(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('Training_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        rst = self.clf(x)\n",
    "        loss = self.criterion(rst, y)\n",
    "        self.log('acc_loss', loss)\n",
    "        y_pred = torch.argmax(rst, 1)\n",
    "        acc = (y_pred.cpu().numpy() == y.cpu().numpy()).mean()\n",
    "        self.log('Acc', acc)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Holdout split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "train_set = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "valid_set = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=len(valid_set), shuffle=False, drop_last=False)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"Acc\",\n",
    "    dirpath=\"weights\",\n",
    "    filename=\"{epoch:d}-{Acc:.6f}\",\n",
    "    save_top_k=1,\n",
    "    mode=\"max\",\n",
    ")\n",
    "ES_callback = EarlyStopping(monitor='Acc', mode='max')\n",
    "\n",
    "model = MLP()\n",
    "trainer = pl.Trainer(gpus=-1, callbacks=[checkpoint_callback, ES_callback], log_every_n_steps=10)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Method\n",
    "Note: 在前一節訓練時，即使用了 7:3 holdout 的切分，因此此節之間載入訓練好的權重進行計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# load best model weight\n",
    "model = MLP.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "# shows metrics for each class\n",
    "def show_multiclass_score(y_pred, y_test, y_label, silent=False):\n",
    "    table_data = list()\n",
    "    table_header = []\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recalls = recall_score(y_test, y_pred, average=None)\n",
    "    precisions = precision_score(y_test, y_pred, average=None)\n",
    "    table_header = ['Label\\Metrics', 'Accuracy', 'Recall', 'Precision']\n",
    "    for name, recall, precision in zip(y_label, recalls, precisions):\n",
    "        table_data.append([name,accuracy, recall, precision])\n",
    "    if not silent:\n",
    "        print(tabulate(table_data, table_header, tablefmt='fancy_grid'))\n",
    "\n",
    "    return accuracy_score(y_test, y_pred), recall_score(y_test, y_pred, average='macro'), precision_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Latex\n",
    "from pprint import pprint\n",
    "\n",
    "# Multi-class\n",
    "y_pred = model(X_valid)\n",
    "print('Multi-class metrics: ')\n",
    "acc, recall, pres = show_multiclass_score(y_pred, y_valid, y_encoder.classes_)\n",
    "display(Latex(f\"$Accuracy = {acc:4f},\\ Average\\ recall = {recall:4f},\\ Average\\ precision = {pres:4f}$\"))\n",
    "\n",
    "# Confusion matrix\n",
    "print('y transform =\\n')\n",
    "pprint(dict(enumerate(y_encoder.classes_, 1)))\n",
    "print('\\nConfusion matrix:')\n",
    "c_m = confusion_matrix(y_valid, y_pred)\n",
    "table_data = list()\n",
    "table_header = ['No.'] + list(range(1, 21))# list(y_encoder.classes_)\n",
    "for idx, row in enumerate(c_m):\n",
    "    table_data.append([table_header[idx+1], *row])\n",
    "print(tabulate(table_data, table_header, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission\n",
    "![alt leader board](Kaggle_leaderboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input\n",
    "with open('2021-nycu-ml-hw5\\\\test.json', 'r') as file:\n",
    "    raw_data = json.load(file)\n",
    "raw_data = [(x['id'], x['ingredients']) for x in raw_data]\n",
    "ids, ingredientss = zip(*raw_data)\n",
    "df = pd.DataFrame({\n",
    "    'id':ids,\n",
    "    'ingredients':ingredientss,\n",
    "})\n",
    "\n",
    "# load best model weight\n",
    "model = MLP.load_from_checkpoint(checkpoint_callback.best_model_path).eval()\n",
    "\n",
    "# Transform\n",
    "df['concat'] = [Porter_Stemmer(s) for s in df['ingredients']] # Porter Stemmer\n",
    "X = vectorizer.transform(df['concat']).toarray().astype(np.float32) # Transform into TF-IDF matrix\n",
    "\n",
    "pred = model(X)\n",
    "Category = y_encoder.inverse_transform(pred)\n",
    "\n",
    "ans = pd.DataFrame({'id':ids, 'Category':Category})\n",
    "print(ans.head())\n",
    "ans.to_csv(\"my_submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
